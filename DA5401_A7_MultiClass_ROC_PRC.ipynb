{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e6a5a61",
   "metadata": {},
   "source": [
    "# DA5401 A7 — Multi-Class Model Selection using ROC & PRC\n",
    "\n",
    "**Objective:** Create a reproducible analysis that compares multiple classifiers on the UCI Landsat Satellite dataset (6 classes) using multi-class ROC and Precision-Recall curves (One-vs-Rest averaging). The notebook below contains code, explanations, plots, and final recommendation.\n",
    "\n",
    "**Contents**\n",
    "\n",
    "1. Data download & preprocessing\n",
    "2. Baseline training & metrics (Accuracy, Weighted F1)\n",
    "3. Multi-class ROC (OvR) — macro-averaged plots and AUCs\n",
    "4. Multi-class Precision-Recall (OvR) — macro-averaged plots and Average Precision\n",
    "5. Synthesis and recommendation\n",
    "\n",
    "**Note:** Run the notebook cells in order. The dataset is downloaded from the UCI repository when you run the cells.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fb53c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "# Run this cell first. If you run in an environment without xgboost installed, the XGBoost cell will skip gracefully.\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, label_binarize\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_curve, auc, precision_recall_curve, average_precision_score\n",
    "from sklearn.metrics import RocCurveDisplay, PrecisionRecallDisplay\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Optional\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGBOOST_AVAILABLE = True\n",
    "except Exception as e:\n",
    "    XGBOOST_AVAILABLE = False\n",
    "\n",
    "print('xgboost available:', XGBOOST_AVAILABLE)\n",
    "print('sklearn version:', __import__('sklearn').__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b89db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file paths — adjust if they’re in a subfolder\n",
    "train_path = \"sat.trn\"\n",
    "test_path = \"sat.tst\"\n",
    "\n",
    "# Column names: 36 features + 1 label\n",
    "col_names = [f'feat_{i}' for i in range(36)] + ['label']\n",
    "\n",
    "train = pd.read_csv(train_path, header=None, delim_whitespace=True)\n",
    "test = pd.read_csv(test_path, header=None, delim_whitespace=True)\n",
    "df = pd.concat([train, test], axis=0).reset_index(drop=True)\n",
    "df.columns = col_names\n",
    "print(\"Shape:\", df.shape)\n",
    "display(df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685fafb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing: drop rows with missing data (if any), filter out 'all types present' if present.\n",
    "# In the UCI Satimage dataset, labels are integers 1..6. If there's a label representing 'all types present', user should remove it.\n",
    "df = df.copy()\n",
    "print('Unique labels before any filtering:', df['label'].unique() if 'label' in df.columns else [])  # helps debug\n",
    "\n",
    "# If the dataset wasn't downloaded, the dataframe will be empty. Guard against that.\n",
    "if df.empty:\n",
    "    raise RuntimeError('Dataframe is empty. Please download the dataset (internet) or provide a local path.')\n",
    "\n",
    "# Convert label to integer, and if there's a label that denotes 'all types present' (e.g., 7) drop it.\n",
    "df['label'] = df['label'].astype(int)\n",
    "# Inspect counts\n",
    "print('Label counts:')\n",
    "display(df['label'].value_counts())\n",
    "\n",
    "# If dataset contains a label not in 1..6, drop it\n",
    "valid_labels = [1,2,3,4,5,6]\n",
    "df = df[df['label'].isin(valid_labels)].reset_index(drop=True)\n",
    "X = df.drop('label', axis=1).values\n",
    "y = df['label'].values\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train/test split (stratified)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.25, random_state=42, stratify=y)\n",
    "print('Train/test shapes:', X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535d7afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the six required models\n",
    "models = {\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'DecisionTree': DecisionTreeClassifier(random_state=42),\n",
    "    'Dummy(Prior)': DummyClassifier(strategy='prior', random_state=42),\n",
    "    'LogisticRegression': LogisticRegression(max_iter=1000, multi_class='ovr', solver='lbfgs'),\n",
    "    'GaussianNB': GaussianNB(),\n",
    "    'SVC': SVC(probability=True, random_state=42)\n",
    "}\n",
    "\n",
    "trained = {}\n",
    "for name, clf in models.items():\n",
    "    clf.fit(X_train, y_train)\n",
    "    trained[name] = clf\n",
    "    print(f'Trained {name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4442d258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline evaluation: Overall Accuracy and Weighted F1\n",
    "from pprint import pprint\n",
    "baseline_metrics = {}\n",
    "for name, clf in trained.items():\n",
    "    y_pred = clf.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1w = f1_score(y_test, y_pred, average='weighted')\n",
    "    baseline_metrics[name] = {'accuracy': acc, 'f1_weighted': f1w}\n",
    "pprint(baseline_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6048f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-class ROC (One-vs-Rest) and macro-averaged ROC plot across models\n",
    "# Binarize labels for OvR\n",
    "classes = np.unique(y_train)\n",
    "y_test_binarized = label_binarize(y_test, classes=classes)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "for name, clf in trained.items():\n",
    "    # For OvR ROC we need probability estimates for each class\n",
    "    if hasattr(clf, \"predict_proba\"):\n",
    "        y_score = clf.predict_proba(X_test)\n",
    "    else:\n",
    "        # For some classifiers that provide decision_function instead:\n",
    "        if hasattr(clf, \"decision_function\"):\n",
    "            try:\n",
    "                y_score = clf.decision_function(X_test)\n",
    "                # If shape is (n_samples,), expand\n",
    "                if y_score.ndim == 1:\n",
    "                    y_score = np.vstack([1 - y_score, y_score]).T\n",
    "            except:\n",
    "                continue\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    # Compute per-class ROC and AUC\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i, cls in enumerate(classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test_binarized[:, i], y_score[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    # Compute macro-average AUC\n",
    "    # First aggregate all FPR\n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in fpr]))\n",
    "    # Then interpolate all ROC at these points\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for i in fpr:\n",
    "        mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "    mean_tpr /= len(classes)\n",
    "    macro_auc = auc(all_fpr, mean_tpr)\n",
    "    plt.plot(all_fpr, mean_tpr, label=f'{name} (macro AUC = {macro_auc:.3f})')\n",
    "\n",
    "plt.plot([0,1],[0,1],'--', linewidth=1, label='Chance')  # diagonal\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate (Recall)')\n",
    "plt.title('OvR Macro-averaged ROC curves for models')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4932253",
   "metadata": {},
   "source": [
    "**ROC Interpretation**\n",
    "\n",
    "- The plot above shows macro-averaged ROC curves (One-vs-Rest interpolation) for each model. Each model's macro AUC is printed in the legend.\n",
    "- Identify the model with the highest macro-averaged AUC from the legend above.\n",
    "- If any model has AUC < 0.5, that implies it's performing worse than random for the averaged OvR decision; possible reasons include the classifier's predictions being inverted for some classes, class imbalance, or a poorly suited model (e.g., Dummy prior for minority classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d1e3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-class Precision-Recall (OvR) and macro-averaged PR plot across models\n",
    "plt.figure(figsize=(10, 8))\n",
    "for name, clf in trained.items():\n",
    "    if hasattr(clf, \"predict_proba\"):\n",
    "        y_score = clf.predict_proba(X_test)\n",
    "    else:\n",
    "        if hasattr(clf, \"decision_function\"):\n",
    "            try:\n",
    "                y_score = clf.decision_function(X_test)\n",
    "                if y_score.ndim == 1:\n",
    "                    y_score = np.vstack([1 - y_score, y_score]).T\n",
    "            except:\n",
    "                continue\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    # Compute per-class PR and average precision\n",
    "    precision = dict()\n",
    "    recall = dict()\n",
    "    ap = dict()\n",
    "    for i, cls in enumerate(classes):\n",
    "        precision[i], recall[i], _ = precision_recall_curve(y_test_binarized[:, i], y_score[:, i])\n",
    "        ap[i] = average_precision_score(y_test_binarized[:, i], y_score[:, i])\n",
    "\n",
    "    # Macro-average precision-recall curve via interpolation on recall\n",
    "    all_recall = np.unique(np.concatenate([recall[i] for i in recall]))\n",
    "    mean_precision = np.zeros_like(all_recall)\n",
    "    for i in recall:\n",
    "        mean_precision += np.interp(all_recall, recall[i][::-1], precision[i][::-1])  # reverse for monotonicity\n",
    "    mean_precision /= len(classes)\n",
    "    macro_ap = np.mean([ap[i] for i in ap])\n",
    "    plt.step(all_recall, mean_precision, where='post', label=f'{name} (mAP = {macro_ap:.3f})')\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('OvR Macro-averaged Precision-Recall curves for models')\n",
    "plt.legend(loc='lower left')\n",
    "plt.grid(True)\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2070f0",
   "metadata": {},
   "source": [
    "**PRC Interpretation**\n",
    "\n",
    "- The legend shows the macro-averaged Average Precision (mAP) for each model.\n",
    "- The model with the highest mAP is the best in terms of precision-recall balance across classes.\n",
    "- Poor models tend to have precision dropping quickly as recall increases because they produce low-quality ranked scores: to increase recall they must accept many false positives, which reduces precision.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95d6f51",
   "metadata": {},
   "source": [
    "## Synthesis & Recommendation\n",
    "\n",
    "Compare rankings from Weighted F1, Macro ROC-AUC, and Macro AP (PRC). Discuss any disagreements (e.g., models with good ROC but low AP) and recommend the best model considering threshold behavior (if you need high precision vs high recall).\n",
    "\n",
    "**Recommendation template:**\n",
    "- If the task values balanced performance across classes and threshold-stability, choose the model with consistently high macro-AUC and macro-AP (e.g., LogisticRegression or SVC depending on results).\n",
    "- If precision for minority classes is critical, prioritize the model with the highest mAP.\n",
    "\n",
    "---\n",
    "\n",
    "### Optional: RandomForest and XGBoost (Brownie points)\n",
    "\n",
    "Add and train RandomForest and XGBoost (if available) and compute the same ROC/PR analyses for comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31d3790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brownie points: RandomForest and XGBoost (optional)\n",
    "extra_models = {}\n",
    "rf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "extra_models['RandomForest'] = rf\n",
    "print('Trained RandomForest')\n",
    "\n",
    "if XGBOOST_AVAILABLE:\n",
    "    xgb_clf = xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
    "    xgb_clf.fit(X_train, y_train)\n",
    "    extra_models['XGBoost'] = xgb_clf\n",
    "    print('Trained XGBoost')\n",
    "else:\n",
    "    print('XGBoost not available in this environment. To use it, install xgboost and re-run this cell.')\n",
    "\n",
    "# You can append extra_models to trained and re-run the ROC/PR plotting cells if desired."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c72deb",
   "metadata": {},
   "source": [
    "### Save & Download\n",
    "\n",
    "Run **File -> Save notebook** in your Jupyter environment. The notebook file can be downloaded from the environment interface or from the path `/mnt/data/DA5401_A7_MultiClass_ROC_PRC.ipynb` if you run this notebook in a standard Jupyter environment.\n",
    "\n",
    "---\n",
    "\n",
    "Good luck! If you want, I can run this notebook here (execute all cells) and attach the generated output, but note that internet access in this execution environment may be restricted for dataset download. If you prefer, I can also produce a pre-filled version with mock/synthetic data so plots are rendered immediately."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
